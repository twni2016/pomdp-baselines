seed: 19
cuda: 0 # use_gpu
env:
  env_type: meta
  env_name: Wind-v0
  max_rollouts_per_task: 1 # k. total steps: 25

  num_tasks: 60
  num_train_tasks: 40
  num_eval_tasks: 20

train:
  # sample complexity: BAMDP horizon * (num_init_rollouts_pool * num_train_tasks
    #  + num_iters * num_tasks_sample * num_rollouts_per_iter)
   # 1000*10*75 = 750k steps
  # rl training steps: num_iters * updates_per_iter

  num_iters: 1000 # number meta-training iterates
  num_init_rollouts_pool: 100 #  before training
  num_rollouts_per_iter: 10  # 1 #
  buffer_size: 1e6

  num_updates_per_iter: 500
  batch_size: 64 # to tune based on sampled_seq_len
  sampled_seq_len: -1 # -1 is all, or positive integer
  sample_weight_baseline: 0.0

eval:
  eval_stochastic: false # also eval stochastic policy
  log_interval: 4 # 10 num of iters
  save_interval: 100 # -1
  log_tensorboard: true

policy:
  arch: gru # [lstm, gru]
  algo: sac # [td3, sac]

  action_embedding_size: 8 # has action input
  state_embedding_size: 32
  reward_embedding_size: 0 # no reward input
  rnn_hidden_size: 64

  dqn_layers: [128, 128]
  policy_layers: [128, 128]
  lr: 0.0003
  gamma: 0.9
  tau: 0.005

  # sac alpha
  entropy_alpha: 0.01 # tend to be det policy...
  automatic_entropy_tuning: true
  alpha_lr: 0.0003
