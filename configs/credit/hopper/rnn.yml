seed: 7
cuda: 0 # use_gpu
env:
  env_type: credit
  env_name: HopperEp-v2

  num_eval_tasks: 10 # num of eval episodes

train:
  # 3000*1000=3M steps
  num_iters: 3000 # number meta-training iterates
  num_init_rollouts_pool: 10 # before training
  num_rollouts_per_iter: 1
  buffer_size: 1e6 # we should full buffer size as VRM

  num_updates_per_iter: 1.0 # equiv to "equal"
  # Let's fix len=64 for fair comparison with VRM
  # VRM uses batch_size=4, which may be too small?
  batch_size: 64 # to tune based on sampled_seq_len
  sampled_seq_len: -1 # all, or positive integer.
  sample_weight_baseline: 0.0 # VRM adds weight

eval:
  eval_stochastic: false # also eval stochastic policy
  log_interval: 10 #num of iters
  save_interval: -1
  log_tensorboard: true

policy:
  arch: lstm # [lstm, gru]
  algo: sac # [td3, sac]

  action_embedding_size: 16
  state_embedding_size: 32
  reward_embedding_size: 0 # as they are constant zeros
  rnn_hidden_size: 128 

  dqn_layers: [256, 256]
  policy_layers: [256, 256]
  lr: 0.0003
  gamma: 0.99
  tau: 0.005

  # sac alpha
  entropy_alpha: 0.01 # tend to be det policy...
  automatic_entropy_tuning: true
  alpha_lr: 0.0003

  # td3 noise 
  ## since we normalize action space to [-1, 1]
  ## the noise std is absolute value
  exploration_noise: 0.1 
  target_noise: 0.2
  target_noise_clip: 0.5